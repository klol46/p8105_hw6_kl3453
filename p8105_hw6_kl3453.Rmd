---
title: "HW6: Markdown file"
author: "Kevin Liu"
date: "2023-12-02"
output: github_document
---

```{r include = FALSE}
library(tidyverse)
library(rvest)
```

# Problem 1

The Washington Post has gathered data on homicides in 50 large U.S. cities and made the data available through a GitHub repository here. You can read their accompanying article here.

```{r }
raw_homicide_data =
  read_csv("./data/homicide-data.csv") |> 
  janitor::clean_names()
```

__Data Process__

Create a city_state variable (e.g. “Baltimore, MD”), and a binary variable indicating whether the homicide is solved.

Omit cities Dallas, TX; Phoenix, AZ; and Kansas City, MO – these don’t report victim race. Also omit Tulsa, AL – this is a data entry mistake. For this problem, limit your analysis those for whom victim_race is white or black. Be sure that victim_age is numeric.

  - Omit: Dallas, TX, Phoeniz, AZ, Kansas City, MO, and Tulsa Al
  - Include only when victim_race == "white" or "black"
  - victim_age has to be numeric 

```{r}
processs_homicide_data = 
  raw_homicide_data |> 
  
  #Create variable "city_state" which is a concatenation of city + state separated by , .
  mutate(city_state = paste(city, state, sep = ", ")) |> 
  
  #Omit Dallas, Phoenix, Kansas Ciry, and Tulsa
  filter(!grepl("Dallas, TX|Phoenix, AZ|Kansas City, MO|Tulsa, AL", city_state)) |> 
  
  #Include victim_race == "white" or "black"
  filter(grepl("White|Black", victim_race)) |> 
  
  #victim_age is numeric
  mutate(victim_age = as.numeric(victim_age))
```

## For the city of Baltimore, MD, use the glm function...

to fit a _logistic regression_ with resolved vs unresolved as the outcome and victim age, sex and race as predictors. 

  - Outcome: Resolved vs. Unresolved (1 or 0 Binary)
  - PRedictors
      - Age: Continuous
      - Sex: Binary
      - Race: Binary 

__Resolved vs. Unresolved (Binary Var)__
```{r}
#Process Data
resolved_data =
  processs_homicide_data |> 
  
  #create binary variable that is 1 or 0 dpeending on if resolved or not
  mutate(resolve = ifelse(disposition %in% c("Closed without arrest", "Open/No arrest"), 0, 1)) 
  
  #NOTE: glm is able to automatically define binary despite the variables being defined as strings. Race + Sex left alone
```

__Logistic Regression__
```{r}
homicide_logistic =
  resolved_data |> 
  glm(resolve ~ victim_age + victim_race + victim_sex, data = _, family = binomial()) 
```

__Tidy__

Save the output of glm as an R object; apply the broom::tidy to this object; 

```{r}
#Extract all parameter estimates from logistic regression
homicide_param_est = 
  homicide_logistic |> 
  broom::tidy() |> 
  mutate(OR = exp(estimate), OR_CI = exp(confint(homicide_logistic))) |>
  select(term, log_OR = estimate, OR, OR_CI, p.value)
```


__OR of Male vs. Female__

Obtain the estimate and confidence interval of the adjusted odds ratio for solving homicides comparing male victims to female victims keeping all other variables fixed.

```{r}
homicide_param_est_male_female = 
  homicide_param_est |> 
  filter(grepl("victim_sexMale", term))
```

- OR Estimate = `r homicide_param_est_male_female$OR`
  - 95% CI: (`r homicide_param_est_male_female$OR_CI`)
  
## Now run glm for each of the cities in your dataset, 

and extract the adjusted odds ratio (and CI) for solving homicides comparing male victims to female victims. Do this within a “tidy” pipeline, making use of purrr::map, list columns, and unnest as necessary to create a dataframe with estimated ORs and CIs for each city.

__Broom function__
```{r}
broom_conf = function(df) {
  broom::tidy(df, conf.int = TRUE)
}
```

```{r}
citystate_OR_CI_hom = 
  resolved_data |> 
  nest(data = -city_state) |> 
  mutate(
    models = map(data, \(df) glm(resolve ~ victim_age + victim_race + victim_sex, data = df, family = binomial())),
    results = map(models, broom_conf)) |> 
  select(-data, -models) |> 
  unnest(results) |> 
  mutate(OR = exp(estimate), lowCI = exp(conf.low), uppCI = exp(conf.high)) |>
  filter(grepl("victim_sexMale", term)) |> 
  select(city_state, OR, lowCI, uppCI)
  
```

Create a plot that shows the estimated ORs and CIs for each city. Organize cities according to estimated OR, and comment on the plot.

```{r}
citystate_OR_CI_hom[order(citystate_OR_CI_hom$OR),] |>  
  mutate(city_state = factor(city_state, levels = city_state)) |> 
  ggplot(aes(x = city_state, y = OR)) +
  geom_errorbar(aes(ymin = lowCI, ymax = uppCI))+
  coord_flip()
```

# Problem 2

```{r}
# Central Park Weather Data
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  select(name, id, everything())

```

The boostrap is helpful when you’d like to perform inference for a parameter / value / summary that doesn’t have an easy-to-write-down distribution in the usual repeated sampling framework. 

We’ll focus on a simple linear regression with _tmax_ as the response with _tmin_ and _prcp_ as the predictors, and are interested in the distribution of two quantities estimated from these data: r^2 + log(b1 * b2)

## Bootstrap

Use 5000 bootstrap samples and, for each bootstrap sample, produce estimates of these two quantities. Plot the distribution of your estimates, and describe these in words. Using the 5000 bootstrap estimates, identify the 2.5% and 97.5% quantiles to provide a 95% confidence interval for r̂ 2 and log(β̂ 0∗β̂ 1).


```{r}
weather_boostrap = 
  weather_df |> 
  select(-name, -id) |> 
  modelr::bootstrap(n = 5000) |> 
  mutate(
    models = map(strap, \(df) lm(tmax ~ tmin + prcp, data = df) ),
    results_tidy = map(models, broom::tidy),
    results_glance = map(models, broom::glance)) |> 
  select(-model) |> 
  unnest(results_tidy)
```

__r^2 Estimate + Conf. Interval__
```{r}
#r^2 estimate plot + CI
r_squared =
  weather_boostrap |> 
  group_by(.id) |>
  select(r.squared) |> 
  distinct()

zval = qnorm(.975)
r_mean = mean(r_squared$r.squared)
r_sd = sd(r_squared$r.squared)
count = nrow(r_squared)
me = zval * (r_sd/sqrt(count))
```

  - r^2 Mean: `r r_mean`
  - r^2 95% CI: (`r r_mean - me`, `r r_mean + me`)

__r^2 plot__
```{r}
r_squared |> 
  ggplot(aes(x = r.squared)) +
  geom_density()
```

__log(beta1 * beta2)__
```{r}
log_b1_b2_df = 
  weather_boostrap |> 
  select(.id, term, estimate) |> 
  pivot_wider(
    names_from = term,
    values_from = estimate
  ) |> 
  mutate(log_est = log(abs(tmin * prcp)))
```

__Plot log(b1 * b2)__
```{r}
log_b1_b2_df |> 
  ggplot(aes(x = log_est)) +
  geom_density()
  
```

# Problem 3